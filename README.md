# E2E Voice Assistant with Pipecat

A real-time voice assistant built using the **Pipecat framework**, orchestrating open-source AI models through E2E Networks infrastructure.

## ğŸ¯ Overview

This project demonstrates a production-ready voice assistant pipeline using:
- **Whisper ASR** for speech-to-text
- **LLaMA 3.3 70B** for intelligent responses
- **Speech5 TTS** for text-to-speech
- **Pipecat Framework** for real-time pipeline orchestration

## ğŸ—ï¸ Architecture

```
User Voice â†’ WebSocket â†’ Pipecat Pipeline â†’ AI Response
                             â†“
                      [Whisper ASR API]
                             â†“
                      [LLaMA 3.3 70B API]
                             â†“
                      [Speech5 TTS API]
                             â†“
                      [Audio Response]
```

## ğŸ“ Project Structure

```
phase_2/
â”œâ”€â”€ services/               # Custom service adapters
â”‚   â”œâ”€â”€ whisper_service.py  # Whisper ASR adapter
â”‚   â”œâ”€â”€ llama_service.py    # LLaMA LLM adapter
â”‚   â””â”€â”€ tts_service.py      # Speech5 TTS adapter
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ voice_assistant.py  # Main Pipecat pipeline
â”œâ”€â”€ web/
â”‚   â””â”€â”€ index.html          # Web interface
â”œâ”€â”€ config.py               # Configuration
â”œâ”€â”€ server.py               # WebSocket server
â”œâ”€â”€ test_local.py           # Local testing
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ setup.sh                # Setup script
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Access to E2E Networks API endpoints
- Microphone and speakers for testing

### Installation

1. Clone the repository:
```bash
git clone <your-repo-url>
cd e2e-voice-assistant
```

2. Run setup script:
```bash
bash setup.sh
```

3. Configure your API endpoints:
```bash
# Copy the example environment file
cp .env.example .env

# Edit .env with your actual endpoints and token
nano .env  # or use any text editor
```

Fill in your actual values:
```env
WHISPER_API=http://your-whisper-endpoint:8000/transcribe
TTS_API=http://your-tts-endpoint:8000/v1/audio/speech
LLAMA_BASE_URL=https://your-llama-endpoint/v1
E2E_TOKEN=your-actual-token-here
```

### Running the Assistant

1. **Start the WebSocket server:**
```bash
source venv/bin/activate
python server.py
```

2. **Open the web interface:**
Navigate to `http://localhost:8080` in your browser

3. **Use the assistant:**
   - Click "Connect" to establish connection
   - Hold the microphone button to speak
   - Release to send your message
   - Listen to the AI response

### Local Testing

Test the pipeline without WebSocket:
```bash
python test_local.py
```

## ğŸ”§ Configuration

Edit `config.py` to customize:
- API endpoints
- Model parameters
- Audio settings
- VAD thresholds

## ğŸ“š How It Works

### 1. Custom Service Adapters
We created Pipecat-compatible adapters for each E2E Networks endpoint:
- `WhisperHTTPService`: Handles ASR via HTTP API
- `LLaMAHTTPService`: Manages LLM via OpenAI-compatible API
- `Speech5HTTPService`: Generates TTS via HTTP API

### 2. Pipeline Orchestration
The Pipecat pipeline manages:
- Audio frame processing
- Voice activity detection (VAD)
- Service coordination
- Interruption handling
- Real-time streaming

### 3. WebSocket Transport
- Real-time bidirectional communication
- Browser-based audio capture
- Low-latency streaming

## ğŸ› ï¸ Development

### Adding New Features

1. **Custom processors:** Create new frame processors in `pipeline/`
2. **Service adapters:** Add new services in `services/`
3. **Transport layers:** Implement new transports (WebRTC, etc.)

### Testing Services

Test individual services:
```python
from services.whisper_service import WhisperHTTPService
# Test your service...
```

## ğŸ“Š Performance

- **Latency Target:** < 500ms total round-trip
- **Audio Format:** 16kHz, mono, 16-bit PCM
- **Supported Browsers:** Chrome, Firefox, Safari, Edge

## ğŸ” Troubleshooting

### Connection Issues
- Verify all API endpoints are accessible
- Check token validity
- Ensure proper CORS configuration

### Audio Issues
- Grant microphone permissions
- Check audio device settings
- Verify sample rate compatibility

### Pipeline Issues
- Check service logs for errors
- Verify frame processing chain
- Monitor WebSocket messages

## ğŸ“ API Reference

### WebSocket Messages

**Client â†’ Server:**
```json
{
  "type": "audio",
  "data": [/* PCM16 audio samples */]
}
```

**Server â†’ Client:**
```json
{
  "type": "transcription",
  "text": "User speech text"
}
```

```json
{
  "type": "response",
  "text": "AI response text"
}
```

```json
{
  "type": "audio",
  "data": [/* PCM16 audio samples */]
}
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- **Pipecat Framework** - Real-time AI pipeline orchestration
- **E2E Networks** - AI infrastructure and model hosting
- **Open-source Models** - Whisper, LLaMA, Speech5

## ğŸ“§ Contact

For questions or support, please contact [your-email]

---

Built with â¤ï¸ using open-source AI models and the Pipecat framework